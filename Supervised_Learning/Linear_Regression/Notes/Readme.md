We want to predict a target variable y (here: car price) using features x (like age, mileage, trim, etc.):

y = Î²0 + Î²1Â·x1 + Î²2Â·x2 + â€¦ + Î²nÂ·xn


y: the actual value (car price in dataset)

Å·: the predicted value (our modelâ€™s guess)

Î²0: the intercept (baseline value when all features are 0)

Î²1, Î²2, â€¦, Î²n: the coefficients (weights) â€” how much each feature contributes to price

x1, x2, â€¦, xn: the input features (age, mileage, LE_flag, Hybrid_flag, etc.)

n: number of features

m: number of data points (cars in dataset)

ðŸ‘‰ Example (car price):

Price = Î²0 + Î²1Â·Age + Î²2Â·Mileage + Î²3Â·LE_Flag + Î²4Â·Hybrid_Flag

Cost Function (MSE)

We measure how wrong our predictions are using Mean Squared Error:

J(Î²) = (1/m) Î£ (Å·(i) - y(i))Â²


J(Î²): the cost (error of the model)

y(i): the actual price of car i

Å·(i): the predicted price of car i

m: total number of cars (rows)

ðŸ‘‰ Intuition: The smaller J(Î²), the better our model fits.

Prediction Function

For each car i:

Å·(i) = Î²0 + Î£ Î²jÂ·xj(i)


Å·(i): predicted price for car i

xj(i): value of feature j for car i

Î²j: weight assigned to feature j

Gradient Descent Updates

We improve the coefficients step by step to minimize J(Î²):

Î²j = Î²j - Î± Â· (âˆ‚J/âˆ‚Î²j)


Î²j: the current weight for feature j

Î±: the learning rate (step size â€” too big = overshoot, too small = slow)

âˆ‚J/âˆ‚Î²j: the gradient (slope of cost function with respect to Î²j)

Gradient Formula
âˆ‚J/âˆ‚Î²j = (2/m) Î£ (Å·(i) - y(i))Â·xj(i)


Measures how much feature j contributes to the error

If gradient is positive â†’ decrease Î²j

If gradient is negative â†’ increase Î²j

Repeat until J(Î²) is minimized
