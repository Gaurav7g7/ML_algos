
Linear Regression Model

We want to predict a target variable 
ğ‘¦
y (here: car price) using features 
ğ‘¥
x (like age, mileage, trim, etc.).

ğ‘¦
=
ğ›½
0
+
ğ›½
1
ğ‘¥
1
+
ğ›½
2
ğ‘¥
2
+
â‹¯
+
ğ›½
ğ‘›
ğ‘¥
ğ‘›
y=Î²
0
	â€‹

+Î²
1
	â€‹

x
1
	â€‹

+Î²
2
	â€‹

x
2
	â€‹

+â‹¯+Î²
n
	â€‹

x
n
	â€‹


ğ‘¦
y: the actual value (car price in dataset).

ğ‘¦
^
y
^
	â€‹

: the predicted value (our modelâ€™s guess).

ğ›½
0
Î²
0
	â€‹

: the intercept (baseline value when all features are 0).

ğ›½
1
,
ğ›½
2
,
â€¦
,
ğ›½
ğ‘›
Î²
1
	â€‹

,Î²
2
	â€‹

,â€¦,Î²
n
	â€‹

: the coefficients (weights) â€” how much each feature contributes to price.

ğ‘¥
1
,
ğ‘¥
2
,
â€¦
,
ğ‘¥
ğ‘›
x
1
	â€‹

,x
2
	â€‹

,â€¦,x
n
	â€‹

: the input features (age, mileage, LE_flag, Hybrid_flag, etc.).

ğ‘›
n: number of features.

ğ‘š
m: number of data points (cars in dataset).

ğŸ‘‰ Example (car price):

Price
=
ğ›½
0
+
ğ›½
1
â‹…
Age
+
ğ›½
2
â‹…
Mileage
+
ğ›½
3
â‹…
LEÂ Flag
+
ğ›½
4
â‹…
HybridÂ Flag
Price=Î²
0
	â€‹

+Î²
1
	â€‹

â‹…Age+Î²
2
	â€‹

â‹…Mileage+Î²
3
	â€‹

â‹…LEÂ Flag+Î²
4
	â€‹

â‹…HybridÂ Flag
ğŸ”¹ Cost Function (MSE)

We measure how wrong our predictions are using Mean Squared Error:

ğ½
(
ğ›½
)
=
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
(
ğ‘¦
^
(
ğ‘–
)
âˆ’
ğ‘¦
(
ğ‘–
)
)
2
J(Î²)=
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

(
y
^
	â€‹

(i)
âˆ’y
(i)
)
2

ğ½
(
ğ›½
)
J(Î²): the cost (error of the model).

ğ‘¦
(
ğ‘–
)
y
(i)
: the actual price of car 
ğ‘–
i.

ğ‘¦
^
(
ğ‘–
)
y
^
	â€‹

(i)
: the predicted price of car 
ğ‘–
i.

ğ‘š
m: total number of cars (rows).

ğŸ‘‰ Intuition: The smaller 
ğ½
(
ğ›½
)
J(Î²), the better our model fits.

ğŸ”¹ Prediction Function

For each car 
ğ‘–
i:

ğ‘¦
^
(
ğ‘–
)
=
ğ›½
0
+
âˆ‘
ğ‘—
=
1
ğ‘›
ğ›½
ğ‘—
ğ‘¥
ğ‘—
(
ğ‘–
)
y
^
	â€‹

(i)
=Î²
0
	â€‹

+
j=1
âˆ‘
n
	â€‹

Î²
j
	â€‹

x
j
(i)
	â€‹


ğ‘¦
^
(
ğ‘–
)
y
^
	â€‹

(i)
: predicted price for car 
ğ‘–
i.

ğ‘¥
ğ‘—
(
ğ‘–
)
x
j
(i)
	â€‹

: value of feature 
ğ‘—
j for car 
ğ‘–
i.

ğ›½
ğ‘—
Î²
j
	â€‹

: weight assigned to feature 
ğ‘—
j.

ğŸ”¹ Gradient Descent Updates

We improve the coefficients step by step to minimize 
ğ½
(
ğ›½
)
J(Î²).

ğ›½
ğ‘—
=
ğ›½
ğ‘—
âˆ’
ğ›¼
â‹…
âˆ‚
ğ½
âˆ‚
ğ›½
ğ‘—
Î²
j
	â€‹

=Î²
j
	â€‹

âˆ’Î±â‹…
âˆ‚Î²
j
	â€‹

âˆ‚J
	â€‹


ğ›½
ğ‘—
Î²
j
	â€‹

: the current weight for feature 
ğ‘—
j.

ğ›¼
Î±: the learning rate (step size â€” too big = overshoot, too small = slow).

âˆ‚
ğ½
âˆ‚
ğ›½
ğ‘—
âˆ‚Î²
j
	â€‹

âˆ‚J
	â€‹

: the gradient (slope of cost function with respect to 
ğ›½
ğ‘—
Î²
j
	â€‹

).

ğŸ”¹ Gradient Formula
âˆ‚
ğ½
âˆ‚
ğ›½
ğ‘—
=
2
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
(
ğ‘¦
^
(
ğ‘–
)
âˆ’
ğ‘¦
(
ğ‘–
)
)
ğ‘¥
ğ‘—
(
ğ‘–
)
âˆ‚Î²
j
	â€‹

âˆ‚J
	â€‹

=
m
2
	â€‹

i=1
âˆ‘
m
	â€‹

(
y
^
	â€‹

(i)
âˆ’y
(i)
)x
j
(i)
	â€‹


Measures how much feature 
ğ‘—
j contributes to the error.

If gradient is positive â†’ decrease 
ğ›½
ğ‘—
Î²
j
	â€‹

.

If gradient is negative â†’ increase 
ğ›½
ğ‘—
Î²
j
	â€‹

.

Repeat until 
ğ½
(
ğ›½
)
J(Î²) is minimized.
